{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2213674",
   "metadata": {},
   "source": [
    "# Train and Test datasets in Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y,\n",
    "test_size= 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0f426",
   "metadata": {},
   "source": [
    "# two modes in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See installed package\n",
    "!pip list\n",
    "# See the directiory\n",
    "%pwd\n",
    "# list of the folder in the directory\n",
    "%ls\n",
    "# See availabe line magics command\n",
    "%lsmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98775e71",
   "metadata": {},
   "source": [
    "# Covariance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate related variables\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy.random import randn\n",
    "from numpy.random import seed\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "# prepare data\n",
    "data1 = 20 * randn(1000) + 100\n",
    "data2 = data1 + (10 * randn(1000) + 50)\n",
    "from numpy import cov\n",
    "covariance = cov(data1, data2)\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03b011",
   "metadata": {},
   "source": [
    "# Correlation Analysis With Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Pearson's correlation\n",
    "from scipy.stats import pearsonr\n",
    "corr, _ = pearsonr(data1, data2)\n",
    "# Note: corr, _ provies only value of r\n",
    "# whereas corr proives r with p-value\n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "# calculate the spearmans's rank correlation between two\n",
    "variables\n",
    "from scipy.stats import spearmanr\n",
    "corr, _ = spearmanr(data1, data2)\n",
    "print('Spearmans correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653759a7",
   "metadata": {},
   "source": [
    "# Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear regression model to the data\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(x.ravel())\n",
    "results = sm.OLS(y,X).fit()\n",
    "results.summary()\n",
    "# Alternatively, using matchine learning tools\n",
    "import scipy.stats\n",
    "import scipy.stats slope, intercept, r, p, stderr =\n",
    "scipy.stats.linregress(x, y)\n",
    "print(slope, intercept, r, p, stderr)\n",
    "line = f'Regression line: y={intercept:.2f}+{slope:.2f}x,\n",
    "r={r:.2f}'\n",
    "line\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, linewidth=0, marker='s', label='Data points')\n",
    "ax.plot(x, intercept + slope * x, label=line)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb93cf",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Polynomial Regression\n",
    "import numpy\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\n",
    "y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))\n",
    "myline = numpy.linspace(1, 22, 100)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(myline, mymodel(myline))\n",
    "plt.show()\n",
    "print(r2_score(y, mymodel(x))) # print R^2\n",
    "# Predict future value\n",
    "speed = mymodel(17)\n",
    "print(speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed71a06",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fa3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "# df = pandas.read_csv(\"cars.csv\")\n",
    "df=pandas.read_csv(\"C:\\\\Users\\\\Dr. M R\n",
    "Karim\\\\Dropbox\\\\a_Lecture Slides\\\\Python\\R code and\n",
    "Data\\\\cars.csv\")\n",
    "X = df[['Weight', 'Volume']]\n",
    "y = df['CO2']\n",
    "# Multiple regession Model,\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X)\n",
    "# X = sm.add_constant(X.ravel())\n",
    "results = sm.OLS(y,X).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively,\n",
    "from sklearn import linear_model # sklearn is a matchine\n",
    "learing package\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "print('Intercept: \\n', regr.intercept_)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "#predict the CO2 emission of a car where the weight is\n",
    "2300kg, and the volume is 1300ccm:\n",
    "predictedCO2 = regr.predict([[2300, 1300]])\n",
    "print(predictedCO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55761aee",
   "metadata": {},
   "source": [
    "# Python Code: OLS for Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3adaecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression Model\n",
    "import statsmodels.api as sm\n",
    "model =sm.OLS(boston.MEDV, sm.add_constant(boston.LSTAT))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7db58",
   "metadata": {},
   "source": [
    "# OLS for Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression Model\n",
    "import statsmodels.api as sm\n",
    "X = boston[['LSTAT', 'CRIM']]\n",
    "model =sm.OLS(boston.MEDV, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "## Alternatively\n",
    "import statsmodels.formula.api as smf\n",
    "# formula: response ~ predictor + predictor\n",
    "est = smf.ols(formula='MEDV ~ LSTAT + CRIM',\n",
    "data=boston).fit()\n",
    "print(est.summary())\n",
    "# In GLM framework\n",
    "Gaussian_model = sm.GLM(boston.MEDV, sm.add_constant(X),\n",
    "family=sm.families.Gaussian()).fit()\n",
    "print(Gaussian_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497b8896",
   "metadata": {},
   "source": [
    "# Python code for Naive Bayes Classication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d053c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "# Data Loading\n",
    "url='https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw\n",
    "/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n",
    "## Confusion Matrix\n",
    "x = df[['sepal_length', 'sepal_width', 'petal_length',\n",
    "'petal_width']]\n",
    "y = df['species']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "test_size=0.25, random_state =3)\n",
    "nb=GaussianNB()\n",
    "nb.fit(x_train,y_train)\n",
    "GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "predictionsNB = nb.predict(x_test)\n",
    "print('Classification\n",
    "Report:\\n',classification_report(y_test, predictionsNB))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(y_test,\n",
    "predictionsNB))\n",
    "print('Accuracy Score:',accuracy_score(y_test,\n",
    "predictionsNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052a37d",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03582b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset=pd.read_csv('temp.csv')\n",
    "dataset=pd.DataFrame(data=dataset.iloc[:,1:6].values,columns=[\"outlook\",\"temprature\",\"humdity\",\"windy\",\"play\"])\n",
    "filter = dataset[\"outlook\"]==\"Rainy\"\n",
    "dataset.where(filter).count()\n",
    "dataset_encoded=dataset.iloc[:,0:5]\n",
    "le=LabelEncoder()\n",
    "for i in dataset_encoded:\n",
    "dataset_encoded[i]=le.fit_transform(dataset_encoded[i])\n",
    "print(dataset_encoded)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9489c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Set\n",
    "X=dataset_encoded.iloc[:,0:4].values\n",
    "#Label Set\n",
    "y=dataset_encoded.iloc[:,4].values\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=1,random_state=2)\n",
    "model=DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train,y_train)\n",
    "if model.predict([[0,1,0,1]])==1:\n",
    "print(\"yes you can play\")\n",
    "else:\n",
    "print(\"no you cant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad7d35",
   "metadata": {},
   "source": [
    "# Plot for the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Data Loading\n",
    "url='https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n",
    "x = df[['sepal_length', 'sepal_width', 'petal_length',\n",
    "'petal_width']]\n",
    "y = df['species']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "test_size=0.25, random_state =3)\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(x_train,y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7590a0",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "x = df[['sepal_length', 'sepal_width', 'petal_length',\n",
    "'petal_width']]\n",
    "y = df['species']\n",
    "feature_cols = ['sepal_length', 'sepal_width',\n",
    "'petal_length', 'petal_width']\n",
    "class_names=['setosa', 'versicolor', 'virginica']\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,\n",
    "filled=True, rounded=True,\n",
    "special_characters=True,feature_names =\n",
    "feature_cols,class_names=class_names)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c1772",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b96c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets\n",
    "y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "clf.predict([[3, 5, 4, 2]])\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets\n",
    "y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,y_train)\n",
    "import pandas as pd\n",
    "feature_imp =\n",
    "pd.Series(clf.feature_importances_,index=iris.feature_names).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0365f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ef7be",
   "metadata": {},
   "source": [
    "# Classication and Regression Tree (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b63697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# load Boston Housing dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "boston = pd.DataFrame(boston_dataset.data,\n",
    "columns=boston_dataset.feature_names)\n",
    "boston['MEDV'] = boston_dataset.target\n",
    "names = boston_dataset.feature_names\n",
    "print(boston.head())\n",
    "print(names)\n",
    "print(boston.shape)\n",
    "array = boston.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1818992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor(max_leaf_nodes = 20)\n",
    "rt = model.fit(X_train, Y_train)\n",
    "rt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "rnd.seed(123458)\n",
    "X_new = X[rnd.randrange(X.shape[0])]\n",
    "X_new = X_new.reshape(1,13)\n",
    "YHat = model.predict(X_new)\n",
    "df = pd.DataFrame(X_new, columns = names)\n",
    "df[\"Predicted Price\"] = YHat\n",
    "df.head(1)\n",
    "from sklearn.metrics import r2_score\n",
    "YHat = model.predict(X_test)\n",
    "r2 = r2_score(Y_test, YHat)\n",
    "print(\"R-Squared = \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb527bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from sklearn import tree\n",
    "import matplotlib.image as mpimg\n",
    "import pydotplus\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "str1 = \"/usr/local/Cellar/graphviz/2.40.1_1/bin/\"\n",
    "sys.path.append(str1)\n",
    "dot_data = io.StringIO()\n",
    "tree.export_graphviz(rt, out_file=dot_data, filled=True,\n",
    "feature_names = names, class_names = 'MEDV')\n",
    "# Draw graph\n",
    "pydotplus.graph_from_dot_data(dot_data.getvalue()).write_png('dt.png')\n",
    "plt.figure(figsize=(100, 100))\n",
    "img = mpimg.imread('dt.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf6047",
   "metadata": {},
   "source": [
    "# KNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "url='https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdc11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.pairplot(df, hue = \"species\", markers=[\"o\", \"s\", \"D\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ca685",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['sepal_length', 'sepal_width', 'petal_length',\n",
    "'petal_width']]\n",
    "y = df['species']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "test_size=0.25, random_state =3)\n",
    "#Fitting K-NN classifier to the training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn= KNeighborsClassifier(n_neighbors=5, metric='minkowski',\n",
    "p=2 )\n",
    "knn.fit(x_train, y_train)\n",
    "predictionsKNN = knn.predict(x_test)\n",
    "print('Classification\n",
    "Report:\\n',classification_report(y_test, predictionsKNN))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(y_test,\n",
    "predictionsKNN))\n",
    "print('Accuracy Score:',accuracy_score(y_test,\n",
    "predictionsKNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659adfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of K for KNN\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k_list = list(range(1,50,2))\n",
    "# creating list of cv scores\n",
    "cv_scores = []\n",
    "# perform 10-fold cross validation\n",
    "for k in k_list:\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "scores = cross_val_score(knn, x_train, y_train, cv=10,\n",
    "scoring='accuracy')\n",
    "cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "plt.figure()\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('The optimal number of neighbors', fontsize=20,\n",
    "fontweight='bold')\n",
    "plt.xlabel('Number of Neighbors K', fontsize=15)\n",
    "plt.ylabel('Misclassification Error', fontsize=15)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(k_list, MSE)\n",
    "plt.show()\n",
    "# finding best k\n",
    "best_k = k_list[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d.\" % best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0751f09",
   "metadata": {},
   "source": [
    "# k fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0886fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing cross-validation from sklearn package.\n",
    "from sklearn import cross_validation\n",
    "# value of K is 10.\n",
    "data = cross_validation.KFold(len(train_set), n_folds=10,\n",
    "indices=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019799f",
   "metadata": {},
   "source": [
    "# k fold cross-validation for Logistic Classication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba27265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "iris = load_iris()\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, iris.data, iris.target)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(logreg, iris.data, iris.target,\n",
    "cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdb9ec",
   "metadata": {},
   "source": [
    "A common way to summarize the cross-validation accuracy is to compute\n",
    "the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score:\n",
    "{:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d940b8",
   "metadata": {},
   "source": [
    "# OLS for Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d355c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression Model\n",
    "import statsmodels.api as sm\n",
    "model =sm.OLS(boston.MEDV, sm.add_constant(boston.LSTAT))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c797923",
   "metadata": {},
   "source": [
    "# OLS for Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae849e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression Model\n",
    "import statsmodels.api as sm\n",
    "X = boston[['LSTAT', 'CRIM']]\n",
    "model =sm.OLS(boston.MEDV, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "## Alternatively\n",
    "import statsmodels.formula.api as smf\n",
    "# formula: response ~ predictor + predictor\n",
    "est = smf.ols(formula='MEDV ~ LSTAT + CRIM',\n",
    "data=boston).fit()\n",
    "print(est.summary())\n",
    "# In GLM framework\n",
    "Gaussian_model = sm.GLM(boston.MEDV, sm.add_constant(X),\n",
    "family=sm.families.Gaussian()).fit()\n",
    "print(Gaussian_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4e047",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression Model\n",
    "import statsmodels.api as sm\n",
    "X = boston[['LSTAT', 'CRIM']]\n",
    "model =sm.OLS(boston.MEDV, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "## Alternatively\n",
    "import statsmodels.formula.api as smf\n",
    "# formula: response ~ predictor + predictor\n",
    "est = smf.ols(formula='MEDV ~ LSTAT + CRIM',\n",
    "data=boston).fit()\n",
    "print(est.summary())\n",
    "# In GLM framework\n",
    "Gaussian_model = sm.GLM(boston.MEDV, sm.add_constant(X),\n",
    "family=sm.families.Gaussian()).fit()\n",
    "print(Gaussian_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "# Intercept and coefficients\n",
    "intercept = model.intercept_[0]\n",
    "coeff_age = model.coef_[0][0]\n",
    "coeff_smoking = model.coef_[0][1]\n",
    "print(f'Intercept (beta0): {intercept}')\n",
    "print(f'Coefficient for Age (beta1): {coeff_age}')\n",
    "print(f'Coefficient for Smoking (beta2): {coeff_smoking}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# Step 1: Data Preparation\n",
    "data = pd.DataFrame({\n",
    "'Age': [40, 55, 48, 60, 35, 50, 45, 58, 42],\n",
    "'Smoking': [0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
    "'HeartAttack': [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "})\n",
    "# Step 2: Model Training\n",
    "X = sm.add_constant(data[['Age', 'Smoking']]) # add a constant\n",
    "term for the intercept\n",
    "y = data['HeartAttack']\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "# Step 3: Model Summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b7955",
   "metadata": {},
   "source": [
    "# Logistic Regression Classier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de66088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "# Convert data set to pandas data frame\n",
    "df=pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "df.head()\n",
    "# See the categories of target variable\n",
    "iris.target_names\n",
    "# take a general notation\n",
    "X = iris.data[:, :4] # we only take the first four features.\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "test_size=0.3, random_state=1234)\n",
    "# Create an instance of Logistic Regression Classifier and\n",
    "fit the data.\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg.fit(X, Y)\n",
    "# See the accuracy rate\n",
    "logreg.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for the test dataset and generate classification\n",
    "report\n",
    "predictionsModel = logreg.predict(X_test)\n",
    "print('Classification\n",
    "Report:\\n',classification_report(Y_test,\n",
    "predictionsModel))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(Y_test,\n",
    "predictionsModel))\n",
    "print('Accuracy Score:',accuracy_score(Y_test,\n",
    "predictionsModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e3341",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbeadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "# See the feature of the data set\n",
    "iris.feature_names\n",
    "# Convert data set to pandas data frame\n",
    "df=pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "df.head()\n",
    "# See the target variable\n",
    "df['target']=iris.target\n",
    "df.head()\n",
    "# See the categories of target variable\n",
    "iris.target_names\n",
    "# take a general notation\n",
    "X = iris.data[:, :4] # we only take the first four features.\n",
    "Y = iris.target\n",
    "# Split dataset into training and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ab1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function for Support vector machine\n",
    "from sklearn.svm import SVC\n",
    "model=SVC()\n",
    "# model=SVC(C=10)\n",
    "# model fitting\n",
    "model.fit(X_train,Y_train)\n",
    "# See the accuracy rate\n",
    "model.score(X_test,Y_test)\n",
    "# predict for the test dataset and generate classification\n",
    "report\n",
    "predictionsModel = model.predict(X_test)\n",
    "print('Classification\n",
    "Report:\\n',classification_report(Y_test,\n",
    "predictionsModel))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(Y_test,\n",
    "predictionsModel))\n",
    "print('Accuracy Score:',accuracy_score(Y_test,\n",
    "predictionsModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b938e",
   "metadata": {},
   "source": [
    "# Boruta Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boruta\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "###initialize Boruta\n",
    "forest = RandomForestRegressor(\n",
    "n_jobs = -1,\n",
    "max_depth = 5\n",
    ")\n",
    "boruta = BorutaPy(\n",
    "estimator = forest,\n",
    "n_estimators = 'auto',\n",
    "max_iter = 100 # number of trials to perform\n",
    ")\n",
    "### fit Boruta (it accepts np.array, not pd.DataFrame)\n",
    "boruta.fit(np.array(X), np.array(y))\n",
    "### print results\n",
    "green_area = X.columns[boruta.support_].to_list()\n",
    "blue_area = X.columns[boruta.support_weak_].to_list()\n",
    "print('features in the green area:', green_area)\n",
    "print('features in the blue area:', blue_area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
